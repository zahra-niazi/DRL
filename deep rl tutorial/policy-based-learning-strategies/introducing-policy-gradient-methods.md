# Introducing Policy-Gradient Methods

## Recap of Policy-Based Methods

In _policy-based methods_, we directly learn to approximate œÄ‚àó without having to learn a value function.

#### The idea is **to parameterize the policy**. For instance, using a neural network œÄŒ∏‚Äã, this policy will output a probability distribution over actions (stochastic policy).

<figure><img src="../.gitbook/assets/stochastic_policy.png" alt=""><figcaption></figcaption></figure>

> Our objective then is **to maximize the performance of the parameterized policy using gradient ascent**.

To do that, we control the parameter that will affect the distribution of actions over a state.

<figure><img src="../.gitbook/assets/policy_based.png" alt=""><figcaption></figcaption></figure>

Consequently, thanks to policy-based methods, we can directly optimize our policy œÄŒ∏‚Äã to output a probability distribution over actions ùúãùúÉ‚üÆùëé|ùë†‚üØ that leads to the best cumulative return.

> To do that, we define an objective function ùêΩ‚üÆùúÉ‚üØ, that is, the expected cumulative reward, and we **want to find the value Œ∏ that maximizes this objective function**.

***

## The difference between policy-based and policy-gradient methods

Policy-gradient methods, what we‚Äôre going to study in this unit, is a subclass of policy-based methods. In policy-based methods, the optimization is most of the time _on-policy_ since for each update, we only use data (trajectories) collected **by our most recent version of** œÄŒ∏.

The difference between these two methods **lies on how we optimize the parameter** Œ∏:

* In _policy-based methods_, we search directly for the optimal policy. We can optimize the parameter Œ∏ **indirectly** by maximizing the local approximation of the objective function with techniques like hill climbing, simulated annealing, or evolution strategies.
* In _policy-gradient methods_, because it is a subclass of the policy-based methods, we search directly for the optimal policy. But we optimize the parameter Œ∏ **directly** by performing the gradient ascent on the performance of the objective function J(Œ∏).

Before diving more into how policy-gradient methods work (the objective function, policy gradient theorem, gradient ascent, etc.), let‚Äôs study the advantages and disadvantages of policy-based methods.
