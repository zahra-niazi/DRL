# Off-policy vs On-policy

## On-Policy Reinforcement Learning:

In on-policy RL, the agent learns from the experiences it gathers while following its current policy. This means that the data used for training the agent comes from the same policy it is trying to improve. It updates its policy based on its current actions and their consequences.

Key characteristics of on-policy RL:

* **Data collection and policy improvement are interleaved:** The agent collects data by interacting with the environment according to the current policy, and then it uses this data to update its policy.
* **Can be more sample-efficient:** Since it learns from its own experiences, on-policy methods can sometimes converge faster with fewer samples compared to off-policy methods.
* **Tends to be more stable:** As the agent is learning from its current policy, it is less likely to face issues related to distributional shift or divergence.
* **However, it might be less explorative:** Since it follows its current policy closely, it may not explore new actions and paths effectively.

## Off-Policy Reinforcement Learning:

In off-policy RL, the agent learns from experiences generated by a different (usually older) policy, often referred to as the "behavior policy." It uses these experiences to learn and improve a different policy called the "target policy." This decoupling of data collection and policy improvement allows for greater flexibility.

Key characteristics of off-policy RL:

* **Data collection and policy improvement are decoupled:** The agent collects data by exploring the environment using a behavior policy, but it uses this data to improve a different target policy.
* **Can be more explorative:** Since the behavior policy can be explorative, off-policy algorithms have the potential to discover new and better actions.
* **Higher potential for reusability:** Once data is collected, it can be reused to train multiple target policies, making it more sample-efficient for later improvements.
* **May suffer from issues with distributional shift:** As the data comes from a different policy, off-policy methods need to handle potential distributional differences between the behavior policy and the target policy.
