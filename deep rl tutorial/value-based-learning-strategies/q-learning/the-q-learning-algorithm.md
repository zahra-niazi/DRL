# The Q-Learning Algorithm

## Step 01: We initialize the Q-table

We need to initialize the Q-table for each state-action pair. **Most of the time, we initialize with values of 0.**



<figure><img src="../../.gitbook/assets/image (17).png" alt=""><figcaption></figcaption></figure>

***

## Step 02: Choose an action using the epsilon-greedy strategy

The epsilon-greedy strategy is a policy that handles the exploration/exploitation trade-off.

The idea is that, with an initial value of É› = 1.0:

* _With probability 1 â€”É›_ : we do **exploitation** (aka our agent selects the action with the highest state-action pair value).
* With probability É›: **we do exploration** (trying random action).



<figure><img src="../../.gitbook/assets/Q-learning-5.png" alt=""><figcaption></figcaption></figure>

At the beginning of the training, **the probability of doing exploration will be huge since É› is very high, so most of the time, weâ€™ll explore.** But as the training goes on, and consequently our **Q-table gets better and better in its estimations, we progressively reduce the epsilon value** since we will need less and less exploration and more exploitation.

***

## Step 03: Perform action ğ´ğ‘¡, get reward ğ‘…ğ‘¡+1 and next state ğ‘†ğ‘¡+1

***

## Step 4: Update ğ‘„âŸ®ğ‘†ğ‘¡, ğ´ğ‘¡âŸ¯

Remember that in TD Learning, we update our policy or value function (depending on the RL method we choose) **after one step of the interaction.**



To produce our TD target, **we used the immediate reward** ğ‘…ğ‘¡**+1â€‹ plus the discounted value of the next state**, computed by finding the action that maximizes the current Q-function at the next state. (We call that bootstrap).

<figure><img src="../../.gitbook/assets/Q-learning-7.png" alt=""><figcaption></figcaption></figure>

#### Therefore, our ğ‘„âŸ®ğ‘†ğ‘¡**,** ğ´ğ‘¡âŸ¯ **update formula goes like this:**

<figure><img src="../../.gitbook/assets/Q-learning-8.png" alt=""><figcaption></figcaption></figure>

#### This means that to update our ğ‘„âŸ®ğ‘†ğ‘¡**,** ğ´ğ‘¡âŸ¯**:**

* We need ğ‘†ğ‘¡**,** ğ´ğ‘¡, ğ‘…ğ‘¡**+1**, ğ‘†ğ‘¡**+1**
* To update our Q-value at a given state-action pair, we use the TD target.

How do we form the TD target?

1. We obtain the reward after taking the action ğ‘…ğ‘¡**+1**
2. To get the **best state-action pair value** for the next state, we use a greedy policy to select the next best action. Note that this is not an epsilon-greedy policy, this will always take the action with the highest state-action value.

Then when the update of this Q-value is done, we start in a new state and select our action **using a epsilon-greedy policy again.**

**This is why we say that Q Learning is an off-policy algorithm.**
