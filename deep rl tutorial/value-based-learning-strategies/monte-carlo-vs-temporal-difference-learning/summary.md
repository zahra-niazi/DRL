# Summary

* With _Monte Carlo_, we update the value function from a complete episode, and so we **use the actual accurate discounted return of this episode.**
* With _TD Learning_, we update the value function from a step, and we replace 𝐺𝑡, which we don’t know, with **an estimated return called the TD target.**

<figure><img src="../../.gitbook/assets/Summary.png" alt=""><figcaption></figcaption></figure>
