# Temporal Difference Learning: learning at each step

#### **Temporal Difference, on the other hand, waits for only one interaction (one step)** ğ‘†ğ‘¡**+1** to form a TD target and update ğ‘‰âŸ®ğ‘†ğ‘¡âŸ¯ using ğ‘…ğ‘¡**+1**â€‹ and ğ›¾âˆ—ğ‘‰âŸ®ğ‘†ğ‘¡**+1**âŸ¯.

#### The idea with **TD is to update the** ğ‘‰âŸ®ğ‘†ğ‘¡âŸ¯**at each step.**

#### But because we didnâ€™t experience an entire episode, we donâ€™t have ğºğ‘¡ (expected return). Instead, **we estimate** ğºğ‘¡ **by adding** ğ‘…ğ‘¡**+1 and the discounted value of the next state.**

#### This is called bootstrapping. Itâ€™s called this **because TD bases its update in part on an existing estimate** ğ‘‰âŸ®ğ‘†ğ‘¡**+1**âŸ¯ **and not a complete sample** ğºğ‘¡**â€‹.**

## **TD approach:**

<figure><img src="../../.gitbook/assets/TD-3.png" alt=""><figcaption></figcaption></figure>

{% hint style="info" %}
This method is called TD(0) or **one-step TD (update the value function after any individual step).**
{% endhint %}

## TD Approach:

At the end of one step (State, Actions, Rewards, and Next States):

* **We have** ğ‘…ğ‘¡**+1**â€‹ and ğ‘†ğ‘¡**+1**
* **We update** ğ‘‰âŸ®ğ‘†ğ‘¡âŸ¯**:**
  * **we estimate** ğºğ‘¡ **by adding** ğ‘…ğ‘¡**+1and the discounted value of the next state.**
  * **TD Target:** ğ‘…ğ‘¡**+1**â€‹ + ğ›¾âˆ—ğ‘‰âŸ®ğ‘†ğ‘¡**+1**âŸ¯

Now we continue to interact with this environment with our updated value function. By running more and more steps, the agent will learn to play better and better.

***

### If we take an example:

<figure><img src="../../.gitbook/assets/TD-2.png" alt=""><figcaption></figcaption></figure>

* We initialize our value function **so that it returns 0 value for each state.**
* Our learning rate (lr) is 0.1 and our discount rate is 1 (= no discount).
* Our mouse begins to explore the environment and takes a random action: **going to the left**
* It gets a reward ğ‘…ğ‘¡**+1**â€‹ = 1 since **it eats a piece of cheese**

We can now update ğ‘‰âŸ®ğ‘†**0**âŸ¯:

* New ğ‘‰âŸ®ğ‘†**0**âŸ¯=ğ‘‰âŸ®ğ‘†**0**âŸ¯+lrâˆ—\[ğ‘…**1**â€‹+ğ›¾âˆ—ğ‘‰âŸ®ğ‘†**0**âŸ¯âˆ’ğ‘‰âŸ®ğ‘†**0**âŸ¯]
* Newğ‘‰âŸ®ğ‘†**0**âŸ¯=0+0.1âˆ—\[1+1âˆ—0â€“0]
* Newğ‘‰âŸ®ğ‘†**0**âŸ¯=0.1

So we just updated our value function for State 0.

Now we **continue to interact with this environment with our updated value function.**
