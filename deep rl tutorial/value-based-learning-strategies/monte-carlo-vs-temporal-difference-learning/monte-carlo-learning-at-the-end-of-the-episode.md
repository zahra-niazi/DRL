# Monte Carlo: learning at the end of the episode

## **Monte Carlo approach:**

Monte Carlo waits until the end of the episode, calculates ğºğ‘¡ (return) and uses it as **a target for updating** ğ‘‰âŸ®ğ‘†ğ‘¡âŸ¯**.**

So it requires a **complete episode of interaction before updating our value function.**

<figure><img src="../../.gitbook/assets/image (20).png" alt=""><figcaption></figcaption></figure>

### If we take an example:

<figure><img src="../../.gitbook/assets/image (10).png" alt=""><figcaption></figcaption></figure>

* We always start the episode **at the same starting point.**
* **The agent takes actions using the policy**. For instance, using an Epsilon Greedy Strategy, a policy that alternates between exploration (random actions) and exploitation.
* We get **the reward and the next state.**
* We terminate the episode if the cat eats the mouse or if the mouse moves > 10 steps.

At the end of the episode:

* **We have a list of State, Actions, Rewards, and Next States tuples.**
* **The agent will sum the total rewards** ğºğ‘¡ (to see how well it did).
* It will then **update** ğ‘‰âŸ®ğ‘†ğ‘¡âŸ¯ **based on the formula.**
* Then **start a new game with this new knowledge better.**

> By running more and more episodes, **the agent will learn to play better and better.**

***

For instance, if we train a state-value function using Monte Carlo:

* We initialize our value function **so that it returns 0 value for each state.**
* Our learning rate (lr) is 0.1 and our discount rate is 1 (= no discount).
* Our mouse **explores the environment and takes random actions.**

<figure><img src="../../.gitbook/assets/image (15).png" alt=""><figcaption></figcaption></figure>

* The mouse made more than 10 steps, so the episode ends.&#x20;
* We have a list of state, action, rewards, next\_state, **we need to calculate the return** ğºğ‘¡.
  * ğºğ‘¡=ğ‘…ğ‘¡**+1**+ğ‘…ğ‘¡**+2**+ğ‘…ğ‘¡**+3**...(for simplicity we donâ€™t discount the rewards).
  * ğºğ‘¡=1+0+0+0+0+0+1+1+0+0Gtâ€‹=1+0+0+0+0+0+1+1+0+0
  * ğºğ‘¡=3
* We can now update ğ‘‰âŸ®ğ‘†**0**âŸ¯:
  * New ğ‘‰âŸ®ğ‘†**0**âŸ¯=ğ‘‰âŸ®ğ‘†**0**âŸ¯+lrâˆ—\[ğºğ‘¡â€”ğ‘‰âŸ®ğ‘†**0**âŸ¯)]
  * New ğ‘‰âŸ®ğ‘†**0**âŸ¯=0+0.1âˆ—\[3â€“0]ğ‘‰âŸ®ğ‘†**0**âŸ¯
  * New ğ‘‰âŸ®ğ‘†**0**âŸ¯=0.3V
